{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for running the code in the notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%aimport RNN_model, RNN_apply_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "If not have run before\n",
    "'''\n",
    "import RNN_apply_ind, os, json, argparse\n",
    "\n",
    "folder_path = r'C:\\Users\\qilei.zhang\\OneDrive - Frontier Airlines\\Documents\\Data\\USconti'\n",
    "seats_file_name = r'\\Schedule_Monthly_Summary_Report_Conti.csv'\n",
    "perf_file_name = r'\\Airline_Performance_Report_Conti.csv'\n",
    "apply_file_name = '\\Schedule_Monthly_Summary_2023Q1234.csv'\n",
    "# Load parameters from the JSON file.\n",
    "if not os.path.exists('parameters.json'):\n",
    "    print(\"parameters.json does not exist, Find the file and put it in the same folder as this file\")\n",
    "with open('parameters.json', 'r') as f:\n",
    "    args = argparse.Namespace(**json.load(f))\n",
    "\n",
    "RNN_apply_ind.main_apply(args, folder_path, seats_file_name, perf_file_name, apply_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "quick look at the result\n",
    "'''\n",
    "import pandas as pd\n",
    "from RNN_apply_ind import DataAna\n",
    "\n",
    "apply_filename='./data/applying_data.csv'\n",
    "ana_df_name = \"./results/data_to_ana_apply.csv\"\n",
    "orig_df = pd.read_csv(apply_filename)\n",
    "\n",
    "ana = DataAna(ana_df_name)\n",
    "ana.merge_previous_data(orig_df)\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Enter airline and route, separated by comma, or 'c' to exit: \")\n",
    "    if user_input.lower() == 'c':\n",
    "        break\n",
    "    try:\n",
    "        airline, route = user_input.split(',')\n",
    "        airline = airline.strip()  # remove possible leading/trailing whitespaces\n",
    "        route = route.strip()  # remove possible leading/trailing whitespaces\n",
    "        ana.plot_prediction(airline, route)\n",
    "    except ValueError:\n",
    "        print(\"Invalid input, please enter the airline and route separated by a comma or 'continue' to proceed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add new performance data and seats data to original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Add new performance data and seats data to original data\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = r'C:\\Users\\qilei.zhang\\OneDrive - Frontier Airlines\\Documents\\Data\\USconti'\n",
    "seats_file_name = r'\\Schedule_Monthly_Summary_Report_Conti.csv'\n",
    "perf_file_name = r'\\Airline_Performance_Report_Conti.csv'\n",
    "\n",
    "seats_file_new = r'\\Schedule_Monthly_Summary_2023Q1.csv'\n",
    "perf_file_new = r'\\Airline_Performance_Report_USconti_2023Q1.csv'\n",
    "\n",
    "seats_df = pd.read_csv(folder_path + seats_file_name)\n",
    "perf_df = pd.read_csv(folder_path + perf_file_name)\n",
    "\n",
    "seats_df_new = pd.read_csv(folder_path + seats_file_new)\n",
    "perf_df_new = pd.read_csv(folder_path + perf_file_new)\n",
    "\n",
    "\n",
    "# Check the column names are the same othewise not proceed\n",
    "if not (seats_df.columns == seats_df_new.columns).all():\n",
    "    print(\"Column names are not the same, please check the data\")\n",
    "    raise ValueError\n",
    "\n",
    "if not (perf_df.columns == perf_df_new.columns).all():\n",
    "    print(\"Column names are not the same, please check the data\")\n",
    "    raise ValueError\n",
    "\n",
    "# Check the data types are the same othewise not proceed\n",
    "if not (seats_df.dtypes == seats_df_new.dtypes).all():\n",
    "    print(\"Column types are not the same, please check the data\")\n",
    "    raise ValueError\n",
    "\n",
    "if not (perf_df.dtypes == perf_df_new.dtypes).all():\n",
    "    print(\"Column types are not the same, please check the data\")\n",
    "    raise ValueError\n",
    "\n",
    "seats_df = pd.concat([seats_df, seats_df_new])\n",
    "perf_df = pd.concat([perf_df, perf_df_new])\n",
    "\n",
    "seats_df.to_csv(folder_path + seats_file_name, index=False)\n",
    "perf_df.to_csv(folder_path + perf_file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test if the dates are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Boundary quarter (<): Q2 2022\n",
      "Test data (>): Q4 2020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Q2 2022', 'Q1 2020', 'Q4 2020')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from RNN_model import calculate_quarters\n",
    "\n",
    "calculate_quarters(pred_num_quarters=3, seq_num=10, start_quarter=\"Q1 2023\", skip_quarters=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatically tune the hyperparameters and record the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Create combinations of parameters to tune\n",
    "'''\n",
    "\n",
    "# Change the parameters.json file to have the following\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "params_to_tune = {\n",
    "    \"learning_rate\": [1e-03, 1e-04],\n",
    "    # \"momentum\": [0.95, 0.98],\n",
    "    # \"batch_size\": [64, 128],\n",
    "    \"epochs\": [20, 30],\n",
    "    # \"n_layers\": [4, 5],\n",
    "    \"drop_prob\": [0.35, 0.4],\n",
    "    \"bidirectional\": [True, False], \n",
    "    \"if_skip\": [True, False], \n",
    "    # \"if_feed_drop\": [True, False], \n",
    "    # \"if_feed_norm\": [True, False],\n",
    "}\n",
    "\n",
    "# Generate all combinations\n",
    "keys, values = zip(*params_to_tune.items())\n",
    "param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(param_combinations)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(resume_training=False, MSE_or_GaussianNLLLoss='GaussianNLLLoss', pred_num_quarters=3, seq_num=10, if_add_time_info=False, learning_rate=0.0001, momentum=0.95, batch_size=64, epochs=20, num_workers=1, shuffle=True, fixed_seed=True, rnn_type='LSTM', n_layers=4, drop_prob=0.35, num_heads=6, start_year=2004, checkpoint_file_name='checkpoint_20.pth', bidirectional=True, if_skip=False, if_feed_drop=True, if_feed_norm=True, start_quarter='Q1 2023', skip_quarters=2, validation_type='Val', tune=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load parameters from the JSON file. Check if the parameters are loaded correctly.\n",
    "with open('parameters.json', 'r') as f:\n",
    "    args = argparse.Namespace(**json.load(f))\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "learning_rate 0.001\n",
      "epochs 20\n",
      "drop_prob 0.35\n",
      "bidirectional True\n",
      "if_skip True\n",
      "1\n",
      "learning_rate 0.001\n",
      "epochs 20\n",
      "drop_prob 0.35\n",
      "bidirectional True\n",
      "if_skip False\n",
      "2\n",
      "learning_rate 0.001\n",
      "epochs 20\n",
      "drop_prob 0.35\n",
      "bidirectional False\n",
      "if_skip True\n",
      "3\n",
      "learning_rate 0.001\n",
      "epochs 20\n",
      "drop_prob 0.35\n",
      "bidirectional False\n",
      "if_skip False\n",
      "4\n",
      "learning_rate 0.001\n",
      "epochs 20\n",
      "drop_prob 0.4\n",
      "bidirectional True\n",
      "if_skip True\n",
      "5\n",
      "learning_rate 0.001\n",
      "epochs 20\n",
      "drop_prob 0.4\n",
      "bidirectional True\n",
      "if_skip False\n",
      "6\n",
      "learning_rate 0.001\n",
      "epochs 20\n",
      "drop_prob 0.4\n",
      "bidirectional False\n",
      "if_skip True\n",
      "7\n",
      "learning_rate 0.001\n",
      "epochs 20\n",
      "drop_prob 0.4\n",
      "bidirectional False\n",
      "if_skip False\n",
      "8\n",
      "learning_rate 0.001\n",
      "epochs 30\n",
      "drop_prob 0.35\n",
      "bidirectional True\n",
      "if_skip True\n",
      "9\n",
      "learning_rate 0.001\n",
      "epochs 30\n",
      "drop_prob 0.35\n",
      "bidirectional True\n",
      "if_skip False\n",
      "10\n",
      "learning_rate 0.001\n",
      "epochs 30\n",
      "drop_prob 0.35\n",
      "bidirectional False\n",
      "if_skip True\n",
      "11\n",
      "learning_rate 0.001\n",
      "epochs 30\n",
      "drop_prob 0.35\n",
      "bidirectional False\n",
      "if_skip False\n",
      "12\n",
      "learning_rate 0.001\n",
      "epochs 30\n",
      "drop_prob 0.4\n",
      "bidirectional True\n",
      "if_skip True\n",
      "13\n",
      "learning_rate 0.001\n",
      "epochs 30\n",
      "drop_prob 0.4\n",
      "bidirectional True\n",
      "if_skip False\n",
      "14\n",
      "learning_rate 0.001\n",
      "epochs 30\n",
      "drop_prob 0.4\n",
      "bidirectional False\n",
      "if_skip True\n",
      "15\n",
      "learning_rate 0.001\n",
      "epochs 30\n",
      "drop_prob 0.4\n",
      "bidirectional False\n",
      "if_skip False\n",
      "16\n",
      "learning_rate 0.0001\n",
      "epochs 20\n",
      "drop_prob 0.35\n",
      "bidirectional True\n",
      "if_skip True\n",
      "17\n",
      "learning_rate 0.0001\n",
      "epochs 20\n",
      "drop_prob 0.35\n",
      "bidirectional True\n",
      "if_skip False\n",
      "18\n",
      "learning_rate 0.0001\n",
      "epochs 20\n",
      "drop_prob 0.35\n",
      "bidirectional False\n",
      "if_skip True\n",
      "19\n",
      "learning_rate 0.0001\n",
      "epochs 20\n",
      "drop_prob 0.35\n",
      "bidirectional False\n",
      "if_skip False\n",
      "20\n",
      "learning_rate 0.0001\n",
      "epochs 20\n",
      "drop_prob 0.4\n",
      "bidirectional True\n",
      "if_skip True\n",
      "21\n",
      "learning_rate 0.0001\n",
      "epochs 20\n",
      "drop_prob 0.4\n",
      "bidirectional True\n",
      "if_skip False\n",
      "22\n",
      "learning_rate 0.0001\n",
      "epochs 20\n",
      "drop_prob 0.4\n",
      "bidirectional False\n",
      "if_skip True\n",
      "23\n",
      "learning_rate 0.0001\n",
      "epochs 20\n",
      "drop_prob 0.4\n",
      "bidirectional False\n",
      "if_skip False\n",
      "24\n",
      "learning_rate 0.0001\n",
      "epochs 30\n",
      "drop_prob 0.35\n",
      "bidirectional True\n",
      "if_skip True\n",
      "25\n",
      "learning_rate 0.0001\n",
      "epochs 30\n",
      "drop_prob 0.35\n",
      "bidirectional True\n",
      "if_skip False\n",
      "26\n",
      "learning_rate 0.0001\n",
      "epochs 30\n",
      "drop_prob 0.35\n",
      "bidirectional False\n",
      "if_skip True\n",
      "27\n",
      "learning_rate 0.0001\n",
      "epochs 30\n",
      "drop_prob 0.35\n",
      "bidirectional False\n",
      "if_skip False\n",
      "28\n",
      "learning_rate 0.0001\n",
      "epochs 30\n",
      "drop_prob 0.4\n",
      "bidirectional True\n",
      "if_skip True\n",
      "29\n",
      "learning_rate 0.0001\n",
      "epochs 30\n",
      "drop_prob 0.4\n",
      "bidirectional True\n",
      "if_skip False\n",
      "30\n",
      "learning_rate 0.0001\n",
      "epochs 30\n",
      "drop_prob 0.4\n",
      "bidirectional False\n",
      "if_skip True\n",
      "31\n",
      "learning_rate 0.0001\n",
      "epochs 30\n",
      "drop_prob 0.4\n",
      "bidirectional False\n",
      "if_skip False\n"
     ]
    }
   ],
   "source": [
    "import RNN_model\n",
    "import RNN_apply_ind, os\n",
    "\n",
    "folder_path = r'C:\\Users\\qilei.zhang\\OneDrive - Frontier Airlines\\Documents\\Data\\USconti'\n",
    "seats_file_name = r'\\Schedule_Monthly_Summary_Report_Conti.csv'\n",
    "perf_file_name = r'\\Airline_Performance_Report_Conti.csv'\n",
    "apply_file_name = 'Schedule_Monthly_Summary_2023Q1234.csv'\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    print(idx)\n",
    "    # Extract parameters from the row\n",
    "    params = row.to_dict()\n",
    "\n",
    "    # Load parameters from the JSON file.\n",
    "    with open('parameters.json', 'r') as f:\n",
    "        args = argparse.Namespace(**json.load(f))\n",
    "\n",
    "    # assign the all parameters from the row to the args object\n",
    "    for key, value in params.items():\n",
    "        setattr(args, key, value)\n",
    "        print(key, value)\n",
    "\n",
    "    # Check types of certain parameters, and transform them if necessary\n",
    "    if not isinstance(args.epochs, int):\n",
    "        args.epochs = int(args.epochs)\n",
    "    if not isinstance(args.batch_size, int):\n",
    "        args.batch_size = int(args.batch_size)\n",
    "    if not isinstance(args.n_layers, int):\n",
    "        args.n_layers = int(args.n_layers)\n",
    "\n",
    "    # Run the model\n",
    "    # Run Training\n",
    "    RNN_model.main_program(args, folder_path, seats_file_name, perf_file_name, tune_folder=str(idx))\n",
    "\n",
    "    # Run validation\n",
    "    RNN_apply_ind.main_apply(args, folder_path, seats_file_name, perf_file_name, apply_file_name, tune_folder=str(idx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download files in batch from Azure Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# define the remote path in databricks and the local download path\n",
    "remote_base_path = 'dbfs:/FileStore/SeatPre/RunModelTune2/'\n",
    "local_base_path = 'C:\\\\Users\\\\qilei.zhang\\\\Downloads\\\\'\n",
    "\n",
    "# define the subfolder and file structure\n",
    "folder_structure = {\n",
    "    \"model\": [\"model.pth\"],\n",
    "    \"results\": [\"error_table_apply.csv\", \"best_route.csv\", \"data_to_ana_apply.csv\"]\n",
    "}\n",
    "\n",
    "# iterate over each numbered folder\n",
    "for i in range(32):  # replace 32 with the maximum numbered folder\n",
    "    for folder, files in folder_structure.items():\n",
    "        # remote_folder_path = os.path.join(remote_base_path, str(i), folder)\n",
    "        remote_folder_path = remote_base_path + str(i) + '/' + folder + '/'\n",
    "        local_folder_path = os.path.join(local_base_path, str(i), folder)\n",
    "        # create the local directory if it doesn't exist\n",
    "        os.makedirs(local_folder_path, exist_ok=True)\n",
    "\n",
    "        # iterate over each file in the folder\n",
    "        for file in files:\n",
    "            # remote_file_path = os.path.join(remote_folder_path, file)\n",
    "            remote_file_path = remote_folder_path + file\n",
    "            local_file_path = os.path.join(local_folder_path, file)\n",
    "\n",
    "            # print the file path\n",
    "            print(remote_file_path, ' ------ ', local_folder_path)\n",
    "\n",
    "            # check if the file already exists locally\n",
    "            if os.path.exists(local_file_path):\n",
    "                print('file already exists, skipping')\n",
    "                continue\n",
    "\n",
    "            # execute the databricks fs cp command\n",
    "            subprocess.run(['databricks', 'fs', 'cp', remote_file_path, local_folder_path], check=True)\n",
    "            print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy preprocessed data to the local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbfs:/FileStore/SeatPre/RunModelTune2/data/applying_data.csv  ------  C:\\Users\\qilei.zhang\\Downloads\\data\n",
      "done\n",
      "dbfs:/FileStore/SeatPre/RunModelTune2/data/cat_mapping.pkl  ------  C:\\Users\\qilei.zhang\\Downloads\\data\n",
      "done\n",
      "dbfs:/FileStore/SeatPre/RunModelTune2/data/embed_dim_mapping.pkl  ------  C:\\Users\\qilei.zhang\\Downloads\\data\n",
      "done\n",
      "dbfs:/FileStore/SeatPre/RunModelTune2/data/le_airlines.pkl  ------  C:\\Users\\qilei.zhang\\Downloads\\data\n",
      "done\n",
      "dbfs:/FileStore/SeatPre/RunModelTune2/data/le_airports.pkl  ------  C:\\Users\\qilei.zhang\\Downloads\\data\n",
      "done\n",
      "dbfs:/FileStore/SeatPre/RunModelTune2/data/scaled_data.csv  ------  C:\\Users\\qilei.zhang\\Downloads\\data\n",
      "done\n",
      "dbfs:/FileStore/SeatPre/RunModelTune2/data/testing_data.csv  ------  C:\\Users\\qilei.zhang\\Downloads\\data\n",
      "done\n",
      "dbfs:/FileStore/SeatPre/RunModelTune2/data/training_data.csv  ------  C:\\Users\\qilei.zhang\\Downloads\\data\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# define the remote path in databricks and the local download path\n",
    "remote_base_path = 'dbfs:/FileStore/SeatPre/RunModelTune2/'\n",
    "local_base_path = 'C:\\\\Users\\\\qilei.zhang\\\\Downloads\\\\'\n",
    "\n",
    "# define the subfolder and file structure\n",
    "folder_structure = {\n",
    "    \"data\": [\"applying_data.csv\", \"cat_mapping.pkl\", \"embed_dim_mapping.pkl\", \n",
    "             \"le_airlines.pkl\", \"le_airports.pkl\", \"scaled_data.csv\", \"testing_data.csv\",\n",
    "             \"training_data.csv\"],\n",
    "}\n",
    "\n",
    "# iterate over each numbered folder\n",
    "for folder, files in folder_structure.items():\n",
    "    # remote_folder_path = os.path.join(remote_base_path, str(i), folder)\n",
    "    remote_folder_path = remote_base_path + folder + '/'\n",
    "    local_folder_path = os.path.join(local_base_path, folder)\n",
    "    # create the local directory if it doesn't exist\n",
    "    os.makedirs(local_folder_path, exist_ok=True)\n",
    "\n",
    "    # iterate over each file in the folder\n",
    "    for file in files:\n",
    "        # remote_file_path = os.path.join(remote_folder_path, file)\n",
    "        remote_file_path = remote_folder_path + file\n",
    "        local_file_path = os.path.join(local_folder_path, file)\n",
    "\n",
    "        # print the file path\n",
    "        print(remote_file_path, ' ------ ', local_folder_path)\n",
    "\n",
    "        # check if the file already exists locally\n",
    "        if os.path.exists(local_file_path):\n",
    "            print('file already exists, skipping')\n",
    "            continue\n",
    "\n",
    "        # execute the databricks fs cp command\n",
    "        subprocess.run(['databricks', 'fs', 'cp', remote_file_path, local_folder_path], check=True)\n",
    "        print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run current forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the provided arguments.\n",
      "-------- Start ----------\n",
      "Will use cpu as device\n",
      "Missing airports database loaded.\n",
      "Major hubs database loaded.\n",
      "Dimension mapping loaded.\n",
      "Applying data exists, will load it\n",
      "Train Boundary quarter (<): Q1 2023\n",
      "Validation Boundary quarter (>): Q1 2020\n",
      "Test data (>): Q4 2020\n",
      "Original data loaded.\n",
      "Main scaler rebuilt.\n",
      "Seat scaler rebuilt.\n",
      "Validation/Test data scaled.\n",
      "Date features created.\n",
      "Validation data prepared.\n",
      "Model loaded\n",
      "Evaluating: batch 100 of 695\n",
      "Evaluating: batch 200 of 695\n",
      "Evaluating: batch 300 of 695\n",
      "Evaluating: batch 400 of 695\n",
      "Evaluating: batch 500 of 695\n",
      "Evaluating: batch 600 of 695\n",
      "There is 5554 routes in the route dictionary\n",
      "Time used: 1.2593866507212321 minutes\n",
      "-------- End ----------\n"
     ]
    }
   ],
   "source": [
    "import RNN_apply_ind, os, json, argparse\n",
    "\n",
    "folder_path = r'C:\\Users\\qilei.zhang\\OneDrive - Frontier Airlines\\Documents\\Data\\USconti'\n",
    "seats_file_name = r'\\Schedule_Monthly_Summary_Report_Conti.csv'\n",
    "perf_file_name = r'\\Airline_Performance_Report_Conti.csv'\n",
    "apply_file_name = '\\Schedule_Monthly_Summary_2023Q1234.csv'\n",
    "# Load parameters from the JSON file.\n",
    "if not os.path.exists('parameters.json'):\n",
    "    print(\"parameters.json does not exist, Find the file and put it in the same folder as this file\")\n",
    "with open('parameters.json', 'r') as f:\n",
    "    args = argparse.Namespace(**json.load(f))\n",
    "\n",
    "key = \"validation_type\"\n",
    "value = \"test\"\n",
    "setattr(args, key, value)\n",
    "\n",
    "RNN_apply_ind.main_apply(args, folder_path, seats_file_name, perf_file_name, apply_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
