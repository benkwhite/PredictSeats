{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for running the code in the notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%aimport RNN_model, RNN_apply_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "If not have run before\n",
    "'''\n",
    "import RNN_apply_ind, os, json, argparse\n",
    "\n",
    "folder_path = r'C:\\Users\\qilei.zhang\\OneDrive - Frontier Airlines\\Documents\\Data\\USconti'\n",
    "seats_file_name = r'\\Schedule_Monthly_Summary_Report_Conti.csv'\n",
    "perf_file_name = r'\\Airline_Performance_Report_Conti.csv'\n",
    "apply_file_name = '\\Schedule_Monthly_Summary_2023Q1234.csv'\n",
    "# Load parameters from the JSON file.\n",
    "if not os.path.exists('parameters.json'):\n",
    "    print(\"parameters.json does not exist, Find the file and put it in the same folder as this file\")\n",
    "with open('parameters.json', 'r') as f:\n",
    "    args = argparse.Namespace(**json.load(f))\n",
    "\n",
    "RNN_apply_ind.main_apply(args, folder_path, seats_file_name, perf_file_name, apply_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "quick look at the result\n",
    "'''\n",
    "import pandas as pd\n",
    "from RNN_apply_ind import DataAna\n",
    "\n",
    "apply_filename='./data/applying_data.csv'\n",
    "ana_df_name = \"./results/data_to_ana_apply.csv\"\n",
    "orig_df = pd.read_csv(apply_filename)\n",
    "\n",
    "ana = DataAna(ana_df_name)\n",
    "ana.merge_previous_data(orig_df)\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Enter airline and route, separated by comma, or 'c' to exit: \")\n",
    "    if user_input.lower() == 'c':\n",
    "        break\n",
    "    try:\n",
    "        airline, route = user_input.split(',')\n",
    "        airline = airline.strip()  # remove possible leading/trailing whitespaces\n",
    "        route = route.strip()  # remove possible leading/trailing whitespaces\n",
    "        ana.plot_prediction(airline, route)\n",
    "    except ValueError:\n",
    "        print(\"Invalid input, please enter the airline and route separated by a comma or 'continue' to proceed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add new performance data and seats data to original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Add new performance data and seats data to original data\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = r'C:\\Users\\qilei.zhang\\OneDrive - Frontier Airlines\\Documents\\Data\\USconti'\n",
    "seats_file_name = r'\\Schedule_Monthly_Summary_Report_Conti.csv'\n",
    "perf_file_name = r'\\Airline_Performance_Report_Conti.csv'\n",
    "\n",
    "seats_file_new = r'\\Schedule_Monthly_Summary_2023Q1.csv'\n",
    "perf_file_new = r'\\Airline_Performance_Report_USconti_2023Q1.csv'\n",
    "\n",
    "seats_df = pd.read_csv(folder_path + seats_file_name)\n",
    "perf_df = pd.read_csv(folder_path + perf_file_name)\n",
    "\n",
    "seats_df_new = pd.read_csv(folder_path + seats_file_new)\n",
    "perf_df_new = pd.read_csv(folder_path + perf_file_new)\n",
    "\n",
    "\n",
    "# Check the column names are the same othewise not proceed\n",
    "if not (seats_df.columns == seats_df_new.columns).all():\n",
    "    print(\"Column names are not the same, please check the data\")\n",
    "    raise ValueError\n",
    "\n",
    "if not (perf_df.columns == perf_df_new.columns).all():\n",
    "    print(\"Column names are not the same, please check the data\")\n",
    "    raise ValueError\n",
    "\n",
    "# Check the data types are the same othewise not proceed\n",
    "if not (seats_df.dtypes == seats_df_new.dtypes).all():\n",
    "    print(\"Column types are not the same, please check the data\")\n",
    "    raise ValueError\n",
    "\n",
    "if not (perf_df.dtypes == perf_df_new.dtypes).all():\n",
    "    print(\"Column types are not the same, please check the data\")\n",
    "    raise ValueError\n",
    "\n",
    "seats_df = pd.concat([seats_df, seats_df_new])\n",
    "perf_df = pd.concat([perf_df, perf_df_new])\n",
    "\n",
    "seats_df.to_csv(folder_path + seats_file_name, index=False)\n",
    "perf_df.to_csv(folder_path + perf_file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test if the dates are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Boundary quarter (<): Q2 2022\n",
      "Test data (>): Q4 2020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Q2 2022', 'Q1 2020', 'Q4 2020')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from RNN_model import calculate_quarters\n",
    "\n",
    "calculate_quarters(pred_num_quarters=3, seq_num=10, start_quarter=\"Q1 2023\", skip_quarters=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatically tune the hyperparameters and record the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Create combinations of parameters to tune\n",
    "'''\n",
    "\n",
    "# Change the parameters.json file to have the following\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "params_to_tune = {\n",
    "    \"learning_rate\": [1e-03, 1e-04],\n",
    "    # \"momentum\": [0.95, 0.98],\n",
    "    # \"batch_size\": [64, 128],\n",
    "    \"epochs\": [20, 30],\n",
    "    # \"n_layers\": [4, 5],\n",
    "    \"drop_prob\": [0.35, 0.4],\n",
    "    \"bidirectional\": [True, False], \n",
    "    \"if_skip\": [True, False], \n",
    "    # \"if_feed_drop\": [True, False], \n",
    "    # \"if_feed_norm\": [True, False],\n",
    "}\n",
    "\n",
    "# Generate all combinations\n",
    "keys, values = zip(*params_to_tune.items())\n",
    "param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(param_combinations)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(resume_training=False, MSE_or_GaussianNLLLoss='GaussianNLLLoss', pred_num_quarters=3, seq_num=10, if_add_time_info=False, learning_rate=0.0001, momentum=0.95, batch_size=64, epochs=20, num_workers=1, shuffle=True, fixed_seed=True, rnn_type='LSTM', n_layers=4, drop_prob=0.35, num_heads=6, start_year=2004, checkpoint_file_name='checkpoint_20.pth', bidirectional=True, if_skip=False, if_feed_drop=True, if_feed_norm=True, start_quarter='Q1 2023', skip_quarters=2, validation_type='Val', tune=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load parameters from the JSON file. Check if the parameters are loaded correctly.\n",
    "with open('parameters.json', 'r') as f:\n",
    "    args = argparse.Namespace(**json.load(f))\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "learning_rate 0.001\n",
      "epochs 20\n",
      "drop_prob 0.35\n",
      "bidirectional True\n",
      "if_skip True\n",
      "1\n",
      "learning_rate 0.001\n",
      "epochs 20\n",
      "drop_prob 0.35\n",
      "bidirectional True\n",
      "if_skip False\n",
      "2\n",
      "learning_rate 0.001\n",
      "epochs 20\n",
      "drop_prob 0.35\n",
      "bidirectional False\n",
      "if_skip True\n",
      "3\n",
      "learning_rate 0.001\n",
      "epochs 20\n",
      "drop_prob 0.35\n",
      "bidirectional False\n",
      "if_skip False\n",
      "4\n",
      "learning_rate 0.001\n",
      "epochs 20\n",
      "drop_prob 0.4\n",
      "bidirectional True\n",
      "if_skip True\n",
      "5\n",
      "learning_rate 0.001\n",
      "epochs 20\n",
      "drop_prob 0.4\n",
      "bidirectional True\n",
      "if_skip False\n",
      "6\n",
      "learning_rate 0.001\n",
      "epochs 20\n",
      "drop_prob 0.4\n",
      "bidirectional False\n",
      "if_skip True\n",
      "7\n",
      "learning_rate 0.001\n",
      "epochs 20\n",
      "drop_prob 0.4\n",
      "bidirectional False\n",
      "if_skip False\n",
      "8\n",
      "learning_rate 0.001\n",
      "epochs 30\n",
      "drop_prob 0.35\n",
      "bidirectional True\n",
      "if_skip True\n",
      "9\n",
      "learning_rate 0.001\n",
      "epochs 30\n",
      "drop_prob 0.35\n",
      "bidirectional True\n",
      "if_skip False\n",
      "10\n",
      "learning_rate 0.001\n",
      "epochs 30\n",
      "drop_prob 0.35\n",
      "bidirectional False\n",
      "if_skip True\n",
      "11\n",
      "learning_rate 0.001\n",
      "epochs 30\n",
      "drop_prob 0.35\n",
      "bidirectional False\n",
      "if_skip False\n",
      "12\n",
      "learning_rate 0.001\n",
      "epochs 30\n",
      "drop_prob 0.4\n",
      "bidirectional True\n",
      "if_skip True\n",
      "13\n",
      "learning_rate 0.001\n",
      "epochs 30\n",
      "drop_prob 0.4\n",
      "bidirectional True\n",
      "if_skip False\n",
      "14\n",
      "learning_rate 0.001\n",
      "epochs 30\n",
      "drop_prob 0.4\n",
      "bidirectional False\n",
      "if_skip True\n",
      "15\n",
      "learning_rate 0.001\n",
      "epochs 30\n",
      "drop_prob 0.4\n",
      "bidirectional False\n",
      "if_skip False\n",
      "16\n",
      "learning_rate 0.0001\n",
      "epochs 20\n",
      "drop_prob 0.35\n",
      "bidirectional True\n",
      "if_skip True\n",
      "17\n",
      "learning_rate 0.0001\n",
      "epochs 20\n",
      "drop_prob 0.35\n",
      "bidirectional True\n",
      "if_skip False\n",
      "18\n",
      "learning_rate 0.0001\n",
      "epochs 20\n",
      "drop_prob 0.35\n",
      "bidirectional False\n",
      "if_skip True\n",
      "19\n",
      "learning_rate 0.0001\n",
      "epochs 20\n",
      "drop_prob 0.35\n",
      "bidirectional False\n",
      "if_skip False\n",
      "20\n",
      "learning_rate 0.0001\n",
      "epochs 20\n",
      "drop_prob 0.4\n",
      "bidirectional True\n",
      "if_skip True\n",
      "21\n",
      "learning_rate 0.0001\n",
      "epochs 20\n",
      "drop_prob 0.4\n",
      "bidirectional True\n",
      "if_skip False\n",
      "22\n",
      "learning_rate 0.0001\n",
      "epochs 20\n",
      "drop_prob 0.4\n",
      "bidirectional False\n",
      "if_skip True\n",
      "23\n",
      "learning_rate 0.0001\n",
      "epochs 20\n",
      "drop_prob 0.4\n",
      "bidirectional False\n",
      "if_skip False\n",
      "24\n",
      "learning_rate 0.0001\n",
      "epochs 30\n",
      "drop_prob 0.35\n",
      "bidirectional True\n",
      "if_skip True\n",
      "25\n",
      "learning_rate 0.0001\n",
      "epochs 30\n",
      "drop_prob 0.35\n",
      "bidirectional True\n",
      "if_skip False\n",
      "26\n",
      "learning_rate 0.0001\n",
      "epochs 30\n",
      "drop_prob 0.35\n",
      "bidirectional False\n",
      "if_skip True\n",
      "27\n",
      "learning_rate 0.0001\n",
      "epochs 30\n",
      "drop_prob 0.35\n",
      "bidirectional False\n",
      "if_skip False\n",
      "28\n",
      "learning_rate 0.0001\n",
      "epochs 30\n",
      "drop_prob 0.4\n",
      "bidirectional True\n",
      "if_skip True\n",
      "29\n",
      "learning_rate 0.0001\n",
      "epochs 30\n",
      "drop_prob 0.4\n",
      "bidirectional True\n",
      "if_skip False\n",
      "30\n",
      "learning_rate 0.0001\n",
      "epochs 30\n",
      "drop_prob 0.4\n",
      "bidirectional False\n",
      "if_skip True\n",
      "31\n",
      "learning_rate 0.0001\n",
      "epochs 30\n",
      "drop_prob 0.4\n",
      "bidirectional False\n",
      "if_skip False\n"
     ]
    }
   ],
   "source": [
    "import RNN_model\n",
    "import RNN_apply_ind, os\n",
    "\n",
    "folder_path = r'C:\\Users\\qilei.zhang\\OneDrive - Frontier Airlines\\Documents\\Data\\USconti'\n",
    "seats_file_name = r'\\Schedule_Monthly_Summary_Report_Conti.csv'\n",
    "perf_file_name = r'\\Airline_Performance_Report_Conti.csv'\n",
    "apply_file_name = 'Schedule_Monthly_Summary_2023Q1234.csv'\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    print(idx)\n",
    "    # Extract parameters from the row\n",
    "    params = row.to_dict()\n",
    "\n",
    "    # Load parameters from the JSON file.\n",
    "    with open('parameters.json', 'r') as f:\n",
    "        args = argparse.Namespace(**json.load(f))\n",
    "\n",
    "    # assign the all parameters from the row to the args object\n",
    "    for key, value in params.items():\n",
    "        setattr(args, key, value)\n",
    "        print(key, value)\n",
    "\n",
    "    # Check types of certain parameters, and transform them if necessary\n",
    "    if not isinstance(args.epochs, int):\n",
    "        args.epochs = int(args.epochs)\n",
    "    if not isinstance(args.batch_size, int):\n",
    "        args.batch_size = int(args.batch_size)\n",
    "    if not isinstance(args.n_layers, int):\n",
    "        args.n_layers = int(args.n_layers)\n",
    "\n",
    "    # Run the model\n",
    "    # Run Training\n",
    "    RNN_model.main_program(args, folder_path, seats_file_name, perf_file_name, tune_folder=str(idx))\n",
    "\n",
    "    # Run validation\n",
    "    RNN_apply_ind.main_apply(args, folder_path, seats_file_name, perf_file_name, apply_file_name, tune_folder=str(idx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run current forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run current forecast\n",
    "# Need to put validation_type to be \"test\" otherwise use \"Val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the provided arguments.\n",
      "-------- Start ----------\n",
      "Will use cpu as device\n",
      "Missing airports database loaded.\n",
      "Major hubs database loaded.\n",
      "Dimension mapping loaded.\n",
      "Applying data exists, will load it\n",
      "Train Boundary quarter (<): Q1 2023\n",
      "Validation Boundary quarter (>): Q1 2020\n",
      "Test data (>): Q4 2020\n",
      "Original data loaded.\n",
      "Main scaler rebuilt.\n",
      "Seat scaler rebuilt.\n",
      "Validation/Test data scaled.\n",
      "Date features created.\n",
      "found number\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "datasets should not be an empty iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m value \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[39msetattr\u001b[39m(args, key, value)\n\u001b[1;32m---> 17\u001b[0m RNN_apply_ind\u001b[39m.\u001b[39;49mmain_apply(args, folder_path, seats_file_name, perf_file_name, apply_file_name)\n",
      "File \u001b[1;32mc:\\Users\\qilei.zhang\\OneDrive - Frontier Airlines\\Documents\\GitHub\\PredictSeats\\RNN_apply_ind.py:1019\u001b[0m, in \u001b[0;36mmain_apply\u001b[1;34m(args, folder_path, seats_file_name, perf_file_name, apply_file_name, tune_folder)\u001b[0m\n\u001b[0;32m   1016\u001b[0m scaled_filename \u001b[39m=\u001b[39m data_format\u001b[39m.\u001b[39mdata_root \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mscaled_data.csv\u001b[39m\u001b[39m'\u001b[39m \n\u001b[0;32m   1017\u001b[0m apply_filename \u001b[39m=\u001b[39m data_format\u001b[39m.\u001b[39mdata_root \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mapplying_data.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m-> 1019\u001b[0m data_format\u001b[39m.\u001b[39;49mload_scaled_data_val(train_filename\u001b[39m=\u001b[39;49m train_filename,\n\u001b[0;32m   1020\u001b[0m                                  scaled_filename\u001b[39m=\u001b[39;49mscaled_filename,\n\u001b[0;32m   1021\u001b[0m                                  apply_filename\u001b[39m=\u001b[39;49mapply_filename,\n\u001b[0;32m   1022\u001b[0m                                  load_apply_data\u001b[39m=\u001b[39;49mload_apply_data, \n\u001b[0;32m   1023\u001b[0m                                  test_date\u001b[39m=\u001b[39;49mapply_data_boundary,\n\u001b[0;32m   1024\u001b[0m                                  on_apply_data\u001b[39m=\u001b[39;49m(validation_type\u001b[39m==\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mVal\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m   1025\u001b[0m                                  start_quarter\u001b[39m=\u001b[39;49mstart_quarter)\n\u001b[0;32m   1026\u001b[0m \u001b[39m# test data is test data starting date. \u001b[39;00m\n\u001b[0;32m   1027\u001b[0m full_dataset \u001b[39m=\u001b[39m data_format\u001b[39m.\u001b[39mfull_df\n",
      "File \u001b[1;32mc:\\Users\\qilei.zhang\\OneDrive - Frontier Airlines\\Documents\\GitHub\\PredictSeats\\RNN_apply_ind.py:148\u001b[0m, in \u001b[0;36mValidation.load_scaled_data_val\u001b[1;34m(self, train_filename, test_filename, scaled_filename, apply_filename, load_apply_data, test_date, on_apply_data, start_quarter)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDate features created.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[39m# self.save_scaled_data_val()  # temporary comment\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \n\u001b[0;32m    147\u001b[0m \u001b[39m# prepare the validation data\u001b[39;00m\n\u001b[1;32m--> 148\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfinal_preparation_val(start_quarter)\n\u001b[0;32m    149\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mValidation data prepared.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\qilei.zhang\\OneDrive - Frontier Airlines\\Documents\\GitHub\\PredictSeats\\RNN_apply_ind.py:244\u001b[0m, in \u001b[0;36mValidation.final_preparation_val\u001b[1;34m(self, start_quarter)\u001b[0m\n\u001b[0;32m    239\u001b[0m     route_df \u001b[39m=\u001b[39m route_df\u001b[39m.\u001b[39msort_values(\u001b[39m\"\u001b[39m\u001b[39mDate_delta\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    240\u001b[0m     datasets\u001b[39m.\u001b[39mappend(FlightDataset(route_df, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseq_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_features,\n\u001b[0;32m    241\u001b[0m                                   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcat_features, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip_quarters,\n\u001b[0;32m    242\u001b[0m                                   time_add\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_add, seats_values\u001b[39m=\u001b[39mseats_list,\n\u001b[0;32m    243\u001b[0m                                   n_future\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_future))\n\u001b[1;32m--> 244\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfull_df \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mConcatDataset(datasets)\n",
      "File \u001b[1;32mc:\\Users\\qilei.zhang\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:225\u001b[0m, in \u001b[0;36mConcatDataset.__init__\u001b[1;34m(self, datasets)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m    224\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatasets \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(datasets)\n\u001b[1;32m--> 225\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatasets) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdatasets should not be an empty iterable\u001b[39m\u001b[39m'\u001b[39m  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatasets:\n\u001b[0;32m    227\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(d, IterableDataset), \u001b[39m\"\u001b[39m\u001b[39mConcatDataset does not support IterableDataset\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: datasets should not be an empty iterable"
     ]
    }
   ],
   "source": [
    "import RNN_apply_ind, os, json, argparse\n",
    "\n",
    "folder_path = r'C:\\Users\\qilei.zhang\\OneDrive - Frontier Airlines\\Documents\\Data\\USconti'\n",
    "seats_file_name = r'\\Schedule_Monthly_Summary_Report_Conti.csv'\n",
    "perf_file_name = r'\\Airline_Performance_Report_Conti.csv'\n",
    "apply_file_name = '\\Schedule_Monthly_Summary_2023Q1234.csv'\n",
    "# Load parameters from the JSON file.\n",
    "if not os.path.exists('parameters.json'):\n",
    "    print(\"parameters.json does not exist, Find the file and put it in the same folder as this file\")\n",
    "with open('parameters.json', 'r') as f:\n",
    "    args = argparse.Namespace(**json.load(f))\n",
    "\n",
    "key = \"validation_type\"\n",
    "value = \"test\"\n",
    "setattr(args, key, value)\n",
    "\n",
    "RNN_apply_ind.main_apply(args, folder_path, seats_file_name, perf_file_name, apply_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
